# 设计题目

本专题一共分为四个部分:

1. 为飞行器设计起落架, 并通过二维码识别, 降落在移动的小车上;
2. 飞过一片有障碍物的环境, 基于视觉抓取圆柱体, 飞行到终点, 并穿过至少三个门;
3. 在人群中基于视觉找到指定的人, 并进行跟踪;
4. 利用PX4飞控在仿真器和真机上控制无人机飞行.

小组分工情况为: 本人主要负责第一部分和第二部分的飞控/视觉部分, 张坤同学负责第二部分的路径规划和第四部分, 王力同学主要负责第三部分.

# 任务目标分析

本人主要负责无人机的飞行控制和物体识别部分. 飞行控制部分中, 任务目标是让无人机能平稳, 快速地飞到目标点. 以实验一为例, 在实验一中, 图像识别模块一旦找到了二维码的具体位置, 就可以传给飞行控制模块, 让无人机飞抵该位置. 其一, 由于小车在不断移动中, 需要无人机的飞行足够迅速, 至少要能跟上小车的移动速度; 其二, 小车的移动也意味着无人机需要实时地不断识别二维码位置, 而无人机如果摄像头与地面有倾角将极大地影响识别位置的精确度, 因此无人机需要在飞行中保持尽可能平稳, 减少晃动, 以方便摄像头的识别.

# 设计方案

## 创新的姿态控制PID方法

默认的代码使用距离为控制参数, 用PID进行控制, 可以通过设置Quadricopter_target的位置, 无人机会自动跟踪, 以达到目标点. 这种方式有两个主要的问题: 其一, 不能直接把Quadricopter_target放置在目标点, 因为如果Quadricopter_target和Quadricopter_base的距离突然变大, 会超出PID的控制范围, 无人机会失去稳定导致坠机; 因此, 只能将Quadricopter_target放置在很近的位置, 以很慢的速度一步一步引导无人机飞行. 其二, 移动过程不稳定. 如果PID参数设置的小, 无人机会花费很长的时间响应Quadricopter_target的变化, 飞行速度会降低, 抗干扰性也不佳; 如果PID设置的过大, 无人机对较小的Quadricopter_target变化都会起很大的反应, 晃动很厉害. 尤其是到达目标点附近时, Quadricopter_target在随着小车不断变化, 需要以非常慢的速度控制无人机的降落, 并且等待其震荡消失后才能继续进行图像识别.

本人对调节PID参数等做了很多尝试, 也实现了一个能够降落在小车上的版本, 不过, 本人认为这种方法其实不够完美. 由于飞机的位置其实只能通过其飞行姿态控制, 而飞行姿态决定的是飞机的加速度, 因此利用加速度直接控制位置实际上跨了几级导数, 可能会造成反馈调节不当, 效果不佳.

位置PID的主要问题分为两部分: 其一, 位置较远时, 无人机会产生很大的倾角导致失控, 显然是面对较大误差, PID控制幅度也很大造成的; 这时如果对加速度和速度做出限制, 让无人机以更稳定的方式到达目的地, 可能对飞行有一些改观, 但是也会牺牲飞行速度. 其二, 位置较近时, PID控制往往会出现震荡, 一段时间后才能慢慢恢复稳定; 这也是PID的特性之一. 由于位置PID只对位置的误差进行优化, 而没有考虑无人机倾角等情况, 震荡是不可避免的. 这种震荡将影响无人机视觉, 或者在避障时容易撞到障碍物. 一种更自然的控制方法是让无人机在接近目标时提前减速并慢慢停住, 然后再以比较低的速度和加速度调整自己的姿态; 当然, 这种方法也会减缓无人机到达目标的速度, 耽误时间. 两个问题相互制约, 也体现了位置PID不易同时适应远/近两种距离的飞行; 并且, 不同的飞行场景, 例如高速巡航时和穿门时, 都有不同的精度和速度要求, 为了适应这两种场景或许需要更复杂的控制逻辑, 或者两套不同的PID参数. 不过, 这种方法的问题实际上都是用位置控制PID, 和控制量加速度略有脱节造成的. 因此, 只要改变PID的输入量, 问题就会迎刃而解.

本组采用另一套PID控制方法: 直接用PID控制无人机的旋转角, 也就是其飞行姿态, 再用额外的逻辑设置无人机的姿态的目标值, 例如如果想向某个方向加速, 则向该方向倾斜. 那么控制逻辑就要分成三部分: 先确定目标位置, 再确定速度, 最后确定加速度. 后两个方面我都采用了类似简化PID的控制方法: 目标位置通过图像识别找到之后, 即可设置一个指向该位置的速度, 速度大小与距离成正比, 距离目标点越近, 速度越小, 使无人机既能以很快的速度巡航, 也能在接近目标时渐渐停留在目标点, 减少在目标处的震荡和调整时间, 达到快速而准确的目的. 加速度的方向则是当前速度和期望速度之差, 大小同样地距离成正比(有最小值), 也能有效地减少震荡, 让无人机精确地停留在目标点. 同时, 可以在不同的任务场景选择不同的速度逻辑: 高速巡航时, 速度上限可以有所提升甚至取消, 加速度也可以大幅提升, 只要不倾覆就可以; 而抓取物体/降落时, 需要对速度有所限制, 同时也要保持无人机倾角较小, 便于图像识别, 也避免降落时倾倒; 穿门/避障时, 也需要对速度有一定的限制, 使无人机能以预设的轨迹做精确的曲线飞行.

这种姿态PID控制方法在实验一和实验二中都取得了很好的效果. 实验一中, 无人机在接近小车上空时, 几乎一直都能保持只有4°以内的倾角, 从而随时都可以进行高精度的图像采集; 同时, 无人机的降落过程也是十分平稳的, 不会出现降落到小车外导致视野里找不到二维码的情况. 另外, 通过几次图像采集中小车的位置, 可以估算出小车的速度, 这样也可以在上述控制逻辑中将"期望速度"加上该速度, 让无人机与小车等速飞行, 保证降落的平稳和成功率.在视频中可以看到, 无人机在识别到二维码后, 在向其飞行的同时也在调整自己的速度, 使其与小车共速, 然后再渐渐降落, 保证了之后图像识别的精确性和降落的平稳性. 实验二中, 抓取物体部分需要无人机降落在物体位置0.05m以内, 机械爪才能抓到物体. 如果要抓住物体中心, 以防止飞行中的抖动使物体脱落, 抓取的精度可能要更高, 达到2厘米左右. 姿态PID控制方式通过速度控制策略的改变, 限制无人机的倾角, 能使无人机在平缓降落的同时不断进行图像识别, 校正位置, 以达到足够高的精度, 成功地抓取到了物体. 同时, 实验二中有大量长距离巡航的过程, 例如, 一开始直飞向Target平台时, 无人机能轻松达到3m/s以上的速度, 节省时间. 甚至在复杂的避障和穿门过程中, 无人机也能保持在2m/s左右的极高速度, 精准而出色地完成任务. 另外, 实验一中, 无人机也能和速度高达10m/s的小车平行飞行并稳定降落.

[footnote: 10m/s为小车在模拟器中设置的参数, 而模拟器中(参数均为dt=25ms, ppf=1)绘制的图表显示小车的速度为1m/s. 出于标准的统一, 这里及后文在提到小车速度时使用模拟器中的参数10m/s, 而其它的速度和距离都采用模拟器中绘制的图标等信息展示出来).

[角度和速度图表/视频]

## 速度控制逻辑

姿态PID只控制飞机的倾角, 因此需要更高层的逻辑控制飞机的速度方向和大小, 以指挥PID完成飞行控制. 下面对这些控制逻辑做详细的说明.

任务一降落在小车上的过程比较简单, 只用了一套速度控制逻辑:

* 飞机在离小车远时期望速度高, 近时期望速度低. 姿态控制PID则做出相应的调整, 速度低于期望速度时加速, 高于期望速度时减速. 这样可以在距离远时快速到达目标, 距离近时及时减速, 防止过冲. 并且减小震荡和倾角, 利于图像识别.
* 如果小车脱离了飞机的视野, 飞机先向小车上一次出现的方向飞行, 同时提高高度, 以获得更大的视野, 寻找小车. 
* 由于图像识别模块已经能把小车的位置以非常高的精度确定下来, 并且飞行的稳定性也保证了图像识别的精确性, 使得我们可以通过两次识别之间的时间和小车的位移来估算小车的速度; 这样就可以让无人机沿着同速度飞行, 也就是, 期望速度加上估算的速度. 这样可以让无人机与小车达到共速, 在空中相对静止的情况下进行微调和降落, 使降落过程有极高的稳定性.

这套逻辑能适应各种初始距离, 各种小车运动方向和速度的降落, 有很高的鲁棒性. 如果小车速度较大, 或者距离较远, 姿态控制PID法只需要让无人机保持在某一个倾角, 就能产生恒定的加速度, 很快就能达到并稳定在足够高的速度, 并且在高速下也拥有和低速一样的稳定性和可控性, 这是位置控制PID很难做到的. 借助精准而鲁棒的的图像识别, 任务一能够在高速运行的小车上成功降落.

任务二的飞行环境比较复杂, 主要分为四个部分:

1. 巡航及避障部分

   这部分只需要遵循路径规划部分给出的路径飞行即可. 

   * 巡航时可以牺牲精确度以换取速度, 加速度(倾角)设一个较高的上限, 而速度不需要设限, 可以达到任意高的速度
   * 而在避障时, 要进入预定的曲线轨迹需要酌情降低速度, 速度和拐弯的角度有关. 这样可以保证划过曲线时提供的加速度足以维持向心力而不侧翻或有比较大的误差.
   * 曲线轨迹内部也在不断进行微调, 以克服积累的误差, 让无人机保持在正确的轨道上.
   * 当然, 到达巡航轨迹的终点时需要酌情减速. 例如, 接近门时, 为保证安全, 要把速度控制在2m/s以内(虽然依然是个很高的速度), 同时抑制相对门的横向速度. 靠近抓取/降落平台时, 则可以提前减速, 并且逐渐减小加速度幅度, 到达目标点(也就是加了高斯模糊的平台位置)一米以内范围即可让无人机保持较小的倾角, 并开始图像识别.

2. 穿门部分

   这部分需要控制速度以保证安全性. 先飞向路径规划模块事先规划好的, 大约在门前两米的位置(如果无遮挡); 在接近门前时做一定的减速, 把横向速度(即从一根门柱向另一根门柱, 对穿门有干扰的速度方向)减到低于2m/s, 纵向速度可以不加限制, 用巡航及避障的正常上限即可. 原因是:

   * 如果先前的速度方向已经接近穿门的方向, 纵向速度会较大, 但是横向速度也会较小, 飞行更容易控制, 可以快速穿过门;
   * 如果先前的速度方向与穿门的方向有较大的夹角, 例如接近90°角甚至锐角, 此时纵向速度很小甚至为负, 横向速度较大, 这时限制住横向速度也能保证穿门的安全性.

   在穿门时, 在常规的姿态PID的基础上, 嵌套了一层类似PID的方法控制飞机位置, 使其循着既定路线前进. 这种方法吸取了PID的优越性: 容错率高, 简洁, 普适性好; 不过也有PID固有的震荡问题, 以及震荡是否收敛的问题.

   本组在穿门部分的控制逻辑主要分为两部分:

   * 规划穿门路线: 穿门这部分的路线实际上并不是一开始规划好就一成不变的. 飞机飞行的路线是当前位置下最方便穿门的线. 一开始规划的路线是最安全的路线: 飞机飞到门前, 然后走一个垂直门方向(纵向)的直线. 但实际穿门时也允许飞机以一个斜角进门.

   * 在姿态PID的底层控制基础上, 采用了一个参数不断变化的类似PID控制的顶层逻辑: 越接近门, D越小, P越大. 这种方法能更好地保证以如此高(2m/s)的速度飞行时的安全性. 因为, 远距离时, 更希望横向的速度越小越好, 可以减少震荡, 位置歪一点也可以斜线进门, P大, D小;但是距离很近, 小于震荡振幅, 马上要进门的时候, 如果依然有较大的位置误差, 情况就会变得十分危险, 希望"不惜一切代价地把"这个误差调整回来, 因此距离近的时候P减小, D增大.

   [图]

   上图为某次飞行任务中的轨迹图. 图中可以看出, 飞机实际上是斜着进门的. 进门的路线是震荡越来越小的曲线, 但是靠近门的地方有一个更加尖锐的拐弯, 并且出门的地方震荡越来越大. 这就是P减小D增大造成的. 靠近门时震荡幅度必须要控制住; 而出了门再怎么震荡也没关系了. 正是结合了这种问题的特殊性, 本组才采取了变参数的PID控制逻辑.

## 起落架设计

[起落架图]

如图, 本人设计了一种略微非主流的矩形起落架. 一对可旋转的矩形支撑起飞机, 连接在飞机两侧的转轴处.  比起置于飞机下方的常规Y字形或A字形起落架, 这种起落架展开时拥有更大的面积, 能提供很高的稳定性. 并且, 折叠收起时, 它的重心和飞机几乎重合, 并且几乎完全不占用额外的空间, 便于飞行的灵活性和平稳性. 由于安装在飞机两侧, 起落架在展开和折叠时完全不干扰摄像头和抓手, 给任务一和任务二带来了很大的便利.

下面对任务二中的起落架做一点展开说明. 任务二按规则是不允许用起落架的, 不过前期调试时使用了起落架协助抓取过程. 在后续工作中提升了飞行稳定性, 也加了一些优化策略, 同时任务二的避障飞行等部分也有很大的优化, 为飞机的抓取和降落争取到了很多时间. 因此, 本任务中的抓取和降落实际上是"从容不迫"的, 牺牲了一些速度以保证成功率, 实际上还有很大的提速空间. 我们曾对优化后不用起落架版本的抓取和飞行做了几百次模拟, 无一失误.

任务二中, 抓取物体时, 借助起落架可以达到更快的抓取速度: 在飞机飞到较低的高度, 并且处于物体的正上方时, 直接张开起落架并降落, 然后停在平台上闭合抓手, 这样可以提供比较稳定的抓取环境, 抓手就算闭合速度很慢, 也能抓到物体. 不过本组实际上并没有借助起落架进行抓取, 而是短暂悬停在物体上空, 这样实际上对高度和位置的控制也有更高的要求. 为了实现抓取最后一个阶段的精确性, 本组尝试了几个优化策略:

1. 在较低的高度不再进行图像识别. 这时候目标的位置已经基本确定了, 只要向固定的目标点降落即可. 在较低的高度, 有时会看不全物体, 造成误差.
2. 控制飞行的平稳性. 飞机在高空就已经飞到了很精确的位置, 在下降过程中不断调整位置, 如果位置误差较大则停止下降. 这样下降时倾角和横向速度都很小, 抓取时有更高的成功率.
3. 各方向误差加权. 误差不再用欧式距离; 因为圆柱体沿轴向的误差比垂直轴向的误差更重要, 垂直轴向方向只要保证圆柱体在抓手下即可, 而沿轴向方向需要拦腰抓住圆柱体才能避免滑落, 因此对这个方向有更高的加权. 这样也显著提升了抓取成功率.
4. $\gamma$角PID控制. 相比其它两个倾角, $\gamma$角控制飞机在水平面上旋转的角度, 在飞行控制里不是很重要, 但是抓取时十分重要. 在都保持水平降落抓取的情况下, 不同的旋转角实际上抓取效果完全不同: 极端的例子是, 如果无旋转抓取则是"拦腰抓起", 十分稳定, 而旋转了90°再抓取则几乎不可能抓起来. 因此, 在助教的帮助下, 我们对$\gamma$角PID进行了调参, 并且规定$\gamma$角过大时不能抓取, 提高了成功率.

这些优化过程将在后面图像识别部分详细介绍.

## 基于视觉的物体识别

这一部分主要包括实验一的二维码识别和实验二的红色圆柱体识别部分. 实验二的T/E平台的识别涉及到圆和直线检测, 将在下一部分介绍.

### 图像识别技术

* 二维码识别部分

  本组没有用双目视觉, 也没有用一些较复杂的直线检测和矩形识别的方法, 因为这些方法需要较多的计算资源, 不利于仿真和实时识别; 更重要的是, 通过最简单的二值化+框出所有黑色像素点+变换到三维世界坐标的方法, 结合一些简单的优化, 就已经可以在识别物体位置时达到非常高的精度了, 没有必要采用复杂的方法.

* 圆柱体识别部分

  圆柱体识别也只需要做二值化筛选出红色像素点, 然后框出所有红色像素点, 变换到三维世界坐标即可. 不过, 由于圆柱体在不断滚动, 识别到的位置也在变化, 不利于精准降落. 为了抵消这些影响, 我们对识别到的结果做了平均, 以达到更高的精确度和稳定性. 这个平均的过程去除了前几次识别的结果, 因为我们担心这些结果是在刚飞抵目标, 还未完全稳定的情况下识别到的, 可能受到飞机的倾角等影响有一定的误差. 在飞机找到物体并开始缓慢降落时, 其姿态会更加平稳, 识别到的物体也有更高的精确度.

* 识别间隔

  由于只采用最简单的算法, 只需要遍历一遍像素点, 因此图像识别耗时很少, 在图像识别的帧里仅带来很短暂的卡顿. 不过尽管如此, 由于图像识别的精度已经足够高, 图像识别没必要每一帧都进行. 首先, 可以规定飞机倾角的限制, 只有在倾角较小(如, $\alpha$, $\beta$都低于4°左右)的时候才进行识别(在任务二的抓取任务中, 避障巡航阶段倾角较大, 而飞抵平台后的降落过程中大部分时间都能保持在这个角度限制之内, 因此这个限制并不怎么影响识别频率); 其次, 可以规定最小识别间隔(例如3帧), 降低识别频率, 以带来更快的仿真效率.

* 摄像头的遮挡情况

  摄像头的遮挡物主要分为三种: 起落架, 抓手和抓起来的物体.

  * 起落架

    本组经过一番构思, 最后设计出了矩形起落架(见上一部分), 由于它的连接点在飞机的两侧而不是下方, 因此, 完全展开和完全折叠的情况下, 对摄像头完全没有干扰. 当然在折叠过程中起落架会经过摄像头视野, 不过这个过程很短暂, 并且在这个过程中也一般是一开始起飞或者最后的降落过程, 也已经不需要飞行视觉了.

  * 抓手

    抓手是一种不可避免的遮挡物. 不过, 抓手只挡住了下面十几个像素长宽的一小条, 对图像识别并无大碍. 不过在二值化+框出所有像素点这个过程中, 抓手(以及一些其它的地面物体或是噪声)可能带来几个像素点的干扰, 影响框选的精度, 因此需要用一些方法去除噪声. 只要用一些简单的方法, 例如直接对像素点分布进行统计, 就可以去除这种零星几个像素点的干扰.

  * 抓起来的物体

    实验二中, 在抓起物体后飞到降落平台, 以及降落的过程中, 物体一直会遮挡一大块视野. 不过物体引入的干扰像素点通过简单的二值化和上述的去除零星噪点的方法就能抵消. 而物体的遮挡对两个摄像头的影响程度是不一样的: Zed0只有左侧边缘会被遮挡, 而Zed1遮挡的位置更靠近中心, 会对图像识别造成很大的干扰. 物体的遮挡也和抓取的位置有关. 如果抓取到物体中间, 物体可能只会遮挡纵向1/3的视野, 而抓到(相对摄像头视野的)下边缘的话, 物体能遮挡掉2/3甚至以上的视野. 因此, 我们决定以Zed0作为主摄像头, 并提了三个策略来消减这种干扰:

    * 抓取时尽量抓到物体中间, 以保证不会滑落, 以及减少视野干扰;
    * 在下降过程中, 如果下降得较低, 物体可能会遮挡住平台, 这时候由于之前已经确定了平台的准确位置, 只要盲降即可.
    * 如果需要更高精度的图像识别, 例如后文的直线和圆检测, 可以通过只取红色通道并对其二值化, 这样就可以消除红色的干扰.


### 任务中的具体实现和优化

本组在基于视觉的物体识别任务中的主要优化点如下:

1. 摄像头位置矫正

   摄像头位置到飞机抓手位置是有一定的横向偏差的. 例如Zed0到抓手的x, y方向都有一分米左右的偏差. 这个偏差对精度要求在厘米级的抓取任务是不可忽略的. 减去这个偏差后, 我们发现抓取成功率有显著提升.

2. 结合视场角算出精确位置

   在通过像素点获得物体在相机里的位置后, 通过视场角即可知道物体的角度; 再结合物体的距离就能确定物体的相对位置. 距离的确定方面可以用双目识别的方法, 不过这个方法在两米以外可能不是很准确, 因此我们还是用高度近似. 由于图像识别时倾角不会太大(最大4°, 正常情况可能只有1°左右), 这个高度带来的误差实际上很小. 这也体现了飞控模块的降低倾角部分的重要性.

3. 超出视野补偿

   如果已知物体的长宽, 那么可以做另一个十分重要的优化: 超出视野补偿. 如果视野内的图像是不完整的(可以通过大小以及框出的物体范围是否到达视野边缘来确定), 我们就可以通过物体的长宽来补全物体. 这在任务一的二维码识别中显得尤为重要, 因为只要看到一个角, 无人机就可以确定二维码位置. 

   ![1565230926622](C:\Users\lm\AppData\Roaming\Typora\typora-user-images\1565230926622.png)

   如图, 在任务一降落任务中, 无人机使用Zed0(右上角), 只能看到一个小角, 而图中无人机用五个小绿球较精确地标出了二维码的四角和中心(由于二维码有些许旋转, 视野内的区域过小, 四角标记可能略有误差), 让无人机拥有更强的追踪能力.

## 直线和圆识别

在任务二中的T/E平台, 以及真实飞行图像的识别和处理, 都用到了直线和圆识别的技术.

### 霍夫变换的基本原理

直线检测和圆检测的主要算法是霍夫变换. 霍夫直线变换的大致原理是:

1. 如果需要, 可以对图像进行边界检测, 以使直线特征更加明显和集中.

2. 将图像中的像素点两两连线, 得到大量的直线. 每根直线都用Hesse标准型
   $$
   r=xcos\theta+ysin\theta
   $$
   表示. 这样, 相似的直线的$(r,\theta)$组就会接近.

3. 将每根直线的$(r,\theta)$在二维空间中表示成一个点, 这个点的空间就称为霍夫空间. 对这些点进行聚类, 筛选掉点数过少的聚类, 就可以得到图像中较长的几根直线.

霍夫圆变换原理类似, 是用三个点构成圆, 并用
$$
(x-a)^2+(y-b)^2=r^2
$$
表示. 那么相似的圆的$(a,b,r)$组就会相近. 将这些圆变换到三维空间的点, 然后进行聚类和筛选, 即可选出点数较多, 较清晰的圆. 同时, 如果有一些先验条件, 还可以对圆的半径进行限制, 使其只寻找半径在一定范围以内的圆, 减少误判率.

### T/E检测中的具体实现

[多图]

T/E平台的检测和区分分为以下几个步骤:

1. 图像预处理

   预处理主要是对图像提取红色通道, 然后进行二值化.

   在降落在E平台时, 会有抓起来的红色物体干扰, 如果用常规的灰度图二值化可能无法排除干扰, 仍然会有一些深色的物体阴影残留在图片中, 影响判断. 这时可以通过提取红色通道后二值化. 这样红色物体的亮度就会较高, 在二值化过程中能干净地去除, 不会对深色的T/E平台标记产生干扰.

   [2图]

2. 检测T/E的包围圈(圆检测)

   最早的版本是先进行直线检测再进行圆检测的. 但是这样操作的话, 由于直线较短, 必须设置较小的阈值, 这样就容易被包围圈的圆形和外界的干扰(如平台的阴影等), 有大量的噪声. 因此, 本组的思路是先检测T/E包围圈, 去除包围圈及其外部的图像干扰, 再进行直线检测.

   ![l3_no_process](D:\prog10\Desktop\lineDetection\l3_no_process.jpg)

   检测包围圈的问题在于: 如果飞行器高度过低, 看不见完整的圆, 圆检测可能得到错误结果或者失效; 或者在视野有遮挡的情况下, 圆检测也可能有小概率失效. 本组采取的策略是:

   * 飞行器高度过低时, 一定是降落了一段时间后的结果, 这段时间内应该能精确地检测到平台的位置, 因此这种情况下只要向上次检测到的平台位置降落即可.
   * 飞行器在高空, 圆检测失效时: 这时只要通过简单的二值化+框选黑色像素点即可定位. 这种框选的方法实际上也能达到很高的精确度. 如果这种方法也找不到平台, 说明视野被遮挡得较严重(例如抓起的红色物体的遮挡), 或者或得到的噪声处理的平台位置偏差过大, 这时只能向上飞, 在更高的高度获取更大的视野, 这样也可以让目标平台在视野中移动, 避开视野的遮挡.

   下图是圆检测处理的图像. 圆形由淡蓝色圈标出.

   [图]

3. 去除圆外区域

   由于包围圈和T/E字样仍有一定间隙, 对圆检测的误差有一定的容忍度, 我们可以安全地去除检测到的圆内部的一部分区域, 以及圆外的所有区域. 由于下一步直线检测也要用二值化的图, 这个图像处理是在二值化的图完成的. 这样, 只要把去除的部分置为白色即可. 处理完毕后得到下图, 可以看到画面中只有E字样了.

4. 进行直线检测

   利用霍夫变换进行直线检测. 检测的阈值需要根据图像大小调整. 经过前几步的预处理, 直线检测的干扰已经去除干净, 这样检测到的直线就几乎没有噪声了.

5. 筛选重复直线

   由于直线检测用的是边缘检测法, 检测结果对每一笔都会有两条直线. 通过简单的相似度判断即可找到相近的直线, 并进行筛选.

6. 用直线数量对T/E区分

   检测出的T和E结果如下.

   显而易见, 由于笔画数不同, T会检测到2根线, 而E会检测到4根, 通过这种方法即可区分T和E.

### 真实图像中的直线和圆检测

本组也对无人机真实飞行过程中采集到的数据做了分析, 对其中的车道和自行车图案做了检测.

检测步骤如下:

1. 二值化

   通过简单的二值化过程得到如下的图片. 可以看到, 跑道和自行车图案十分清晰地呈现出来, 而右侧受到起落架遮挡的部分在二值化过程中和其它物体的黑色混为一体, 减小了干扰; 然而, 这个起落架依然切断了图中的跑道右侧. 如果对这一时刻的双目图像进行三维重建(由本组王力同学完成), 会造成比较大的匹配错误和形变误差, 导致效果不甚理想.

   ![1565243540837](C:\Users\lm\AppData\Roaming\Typora\typora-user-images\1565243540837.png)

2. 直线检测

   通过与上文类似的直线检测算法即可.

   ![1565243636366](C:\Users\lm\AppData\Roaming\Typora\typora-user-images\1565243636366.png)

   这是直接进行直线检测后的结果. 需要注意的是, 由于真实图像和模拟器图像不同, 边界并不是平直的, 因此会检测出大量平行的直线. 通过再次聚类或更简单的相似判断算法即可去除这些直线.

3. 圆检测

   下图是圆检测的结果. 通过设置较严格的阈值, 可以排除噪声的干扰. 如果只对自行车轮感兴趣, 可以设置阈值为40~70,  就可以只检测到自行车两个车轮了(如下图)

   ![1565243715375](C:\Users\lm\AppData\Roaming\Typora\typora-user-images\1565243715375.png)

4. 图像补全

   直线检测结束后, 通过简单的填充直线间的部分, 即可延长跑道, 修复起落架的遮挡. 经过这种预处理后, 双目视觉就可以不受起落架的干扰, 完成质量更高的三维重建.

   ![1565244181411](C:\Users\lm\AppData\Roaming\Typora\typora-user-images\1565244181411.png)

# 实验结果

下面具体地阐述任务一, 任务二的实验情况.

## 任务一

任务一的实验视频可以在附件中找到. 该任务是无人机追踪10m/s的小车并降落的过程.

下面对这一过程中的细节和实现难点进行分析.

### 第一次识别

任务一开始无人机是看不到小车的. 这时无人机会先稳定自己的姿态, 进行小规模晃动, 如果一段时间内依然识别不到, 无人机会提高飞行高度.

由于之前实现了超出边界补偿的算法, 无人机只识别到了一个小角即可迅速确定小车位置, 并快速靠近开始跟踪.

![recording_2019_08_08-14_25-22_Moment](D:\prog10\Desktop\recording_2019_08_08-14_25-22_Moment.jpg)

上图是无人机捕获到小车位置的瞬间. 这里和上文一样, 用绿色小球标志无人机的位置, 二维码的位置和中心点. 由图中看出无人机成功地标出了小车很精确的位置. 虽然二维码可能只出现了几个像素点大小(不过如果将小车平台也加入识别的范围, 出现的面积会更大), 但是至少在模拟器环境中, 这已经比噪声要大很多, 而视野内并没有其它任何更像小车的物体, 因此可以安全地判断为小车. 

### 接近阶段

无人机快速加速接近目标时, 尤其是加速阶段, 飞行的倾角会使摄像头偏向与前进方向相反的方向, 这会导致短暂的失去目标. 而由于只识别到了一次, 无人机也无法计算小车的速度. 为了更快跟住目标, 无人机在飞近目标时就开始减速, 以使摄像头向前进方向, 也就是小车上次出现的方向偏斜, 并且允许无人机在较大倾角情况下也进行图像识别. 

![第二次识别](D:\prog10\Desktop\第二次识别.jpg)

上图为第二次识别到小车的瞬间. 可以看到, 借助姿态控制PID和对应的速度控制逻辑, 无人机经过短暂的加速倾斜后就开始抑制倾角进行平飞, 导致在图中位置时, 无人机虽然刚走了约1/3的路程, 但姿态已经接近平飞, 使其能抓住机会, 第二次看到小车的位置. 不过, 由于这两次检测可能都有较大的误差, 无人机此时还没有估算小车速度并依据其飞行, 而是先飞到第二次检测到的位置, 等待下一次检测.

下面呈现出无人机的飞行目的地和无人机自身位置的距离(目标距离, 单位:米)随时间(单位:秒)的变化关系.

![任务一误差距离](D:\prog10\Desktop\任务一误差距离.png)

[坐标]

从这张图可以更清晰地看到前期的图像识别和飞行过程. 第一次识别到后, 目标距离变为2.5米, 无人机开始加速, 目标距离呈二次曲线状下降; 在2秒左右无人机第二次检测到目标, 目标距离有微小的变化, 无人机调整方向, 继续以较高的速度飞行; 而第三次检测到目标则是3.2秒左右处. 无人机由于没有计算小车速度, 其位置和小车的距离增大到1.5米, 但此时无人机已经接近平稳飞行, 并结合三次的位置信息, 无人机可以推测出小车的位置, 后续的飞行就可以在期望速度上加上小车的速度, 以便与其一起平飞.

### 微调和降落阶段

![第三次识别](D:\prog10\Desktop\第三次识别.jpg)

这是第三次检测时的飞行状态. 无人机的速度已经近似小车的速度, 因此后续只要将自己的速度加到与之共速(10m/s), 然后通过微调缩短距离并降落即可. 此后无人机基本保持在小车上空, 检测到小车的频率愈发频繁. 

降落则通过无人机到小车的距离来控制. 当无人机距离小车较远时, 应保持高度甚至提升高度(如果跟丢小车), 以避免降得过低, 视野过小, 找不到小车的位置; 而距离小车较近时则可以降低高度, 以尽快降落在小车上. 这一逻辑在本组的实现中使用阈值函数来描述. 通过预先设置距离-高度的阈值函数$f$, 无人机只要与小车的位置缩短到某距离$x$, 就被允许降低到高度$f(x)$. 同时, 在跟踪过程中, 无人机也在实时检测小车位置, 使得小车突然转向时无人机也能跟住小车. 并且, 无人机通过新检测到的信息不断修正小车的估计速度, 并最终飞抵小车上空较低, 距离也很近的位置上, 准备降落.

### 最终降落阶段

如果无人机直接在飞行中降低高度直到接触小车, 以这种方式降落, 可能会降落得不稳, 造成倾覆. 因为小车以10m/s的高速运行, 无人机与小车的速度误差也会较大, 在接触的瞬间可能给无人机带来较大的转矩, 使其侧翻; 同时, 无人机在飞行过程中也在不断调整自身的姿态, 有可能出现较大的倾角, 也会使其容易侧翻. 因此, 在降落前需要最终的稳定阶段, 在这一阶段无人机迅速将自身的姿态调至水平, 经过短暂的延迟后, 降低推力, 完成降落. 

在我们的控制逻辑中, 如果无人机的高度低于某个阈值(如0.8米), 并且位置也与小车很相近, 就会进入这一最终降落阶段, 准备降落. 由于无人机之前一直在计算并模仿小车的速度方向和大小, 因此这一阶段就算不进行任何图像识别和位置调整, 仅依靠惯性飞行, 在短时间内也不会与小车拉开太大的误差. 并且, 这一阶段无人机的飞行高度较低, 经常无法看全完整的二维码, 图像识别的结果会有较大的偏差, 对无人机产生误导. 因此, 我们在飞行过程的结尾加上了这个阶段, 以完善无人机的最后一段降落过程.

















如上所述, 本人通过另一种PID控制方法, 实现了无人机更平稳快速的飞行. 因此, 结合另外两名同学的工作, 实验一二已经基本完成. 后续可以优化的空间有:

* 实验二中无人机更快更准确的降落; (需要和图像识别模块配合)
* 实验二在避障和穿门时, 不减速, 规划出一条平滑曲线, 使得能够直接在高速运行时调整速度方向. (需要和路径规划模块配合).

# 参考文献

V-Rep使用手册 http://www.coppeliarobotics.com/helpFiles

霍夫变换 https://en.wikipedia.org/wiki/Hough_transform